{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f960effd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import time\n",
    "\n",
    "env = gym.make('CartPole-v1', render_mode='human')\n",
    "\n",
    "\n",
    "observation, info = env.reset(seed=42)\n",
    "\n",
    "for i in range(100):\n",
    "    observation, reward, terminated, truncated, info = env.step(env.action_space.sample())\n",
    "\n",
    "    print(i)\n",
    "    if terminated or truncated:\n",
    "        time.sleep(2)\n",
    "        observation, info = env.reset()\n",
    "        break\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d436ea43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from collections import deque\n",
    "\n",
    "np.random.seed(1)\n",
    "tf.random.set_seed(1)\n",
    "Transition = namedtuple(\n",
    "'Transition', ('state', 'action', 'reward',\n",
    "'next_state', 'done'))\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(\n",
    "        self, env, discount_factor=0.95,\n",
    "        epsilon_greedy=1.0, epsilon_min=0.01,\n",
    "        epsilon_decay=0.995, learning_rate=1e-3,\n",
    "        max_memory_size=2000):\n",
    "        self.enf = env\n",
    "        self.state_size = env.observation_space.shape[0]\n",
    "        self.action_size = env.action_space.n\n",
    "        self.memory = deque(maxlen=max_memory_size)\n",
    "        self.gamma = discount_factor\n",
    "        self.epsilon = epsilon_greedy\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.lr = learning_rate\n",
    "        self._build_nn_model()\n",
    "        \n",
    "    def _build_nn_model(self,n_layers=3):\n",
    "        self.model = tf.keras.Sequential()\n",
    "        ## Hidden layers\n",
    "        \n",
    "\n",
    "        for n in range(n_layers-1):\n",
    "            self.model.add(tf.keras.layers.Dense(\n",
    "            units=32, activation='relu'))\n",
    "        self.model.add(tf.keras.layers.Dense(\n",
    "        units=32, activation='relu'))\n",
    "        ## Last layer\n",
    "        self.model.add(tf.keras.layers.Dense(\n",
    "        units=self.action_size))\n",
    "            ## Build & compile model\n",
    "        self.model.build(input_shape=(None, self.state_size))\n",
    "        self.model.compile(\n",
    "        loss='mse',\n",
    "        optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=self.lr))\n",
    "            \n",
    "    def save_model(self, e):\n",
    "        # Save the weights\n",
    "        self.model.save_weights('./checkpoints3/DQN_checkpoint_'+str(e))\n",
    "        \n",
    "        \n",
    "    def load_model(self,e):\n",
    "        self.model.load_weights('./checkpoints3/DQN_checkpoint_'+str(e))\n",
    "        \n",
    "  \n",
    "            \n",
    "    def remember(self, transition):\n",
    "        self.memory.append(transition)\n",
    "        \n",
    "    def choose_action(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        \n",
    "        \n",
    "        q_values = self.model.predict(state, verbose = 0)[0]\n",
    "        return np.argmax(q_values) # returns action\n",
    "    \n",
    "    def _learn(self, batch_samples):\n",
    "        batch_states, batch_targets = [], []\n",
    "        for ti, transition in enumerate(batch_samples):\n",
    "            s, a, r, next_s, done = transition\n",
    "            if done:\n",
    "                target = r\n",
    "            else:\n",
    "                target = (r +\n",
    "                    self.gamma * np.amax(\n",
    "                    self.model.predict(next_s,verbose = 0)[0]\n",
    "                    )\n",
    "                    )\n",
    "                \n",
    "            \n",
    "            target_all = self.model.predict(s,verbose = 0)[0]\n",
    "            \n",
    "            target_all[a] = target\n",
    "            batch_states.append(s.flatten())\n",
    "            \n",
    "#             print(s.shape,target_all.shape)\n",
    "            \n",
    "            batch_targets.append(target_all)\n",
    "            self._adjust_epsilon()\n",
    "            \n",
    "            \n",
    "            \n",
    "        return self.model.fit(x=np.array(batch_states),\n",
    "                            y=np.array(batch_targets),\n",
    "                            epochs=1,\n",
    "                            verbose=0)\n",
    "    \n",
    "    def _adjust_epsilon(self):\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "        \n",
    "    def replay(self, batch_size):\n",
    "        samples = random.sample(self.memory, batch_size)\n",
    "        history = self._learn(samples)\n",
    "        return history.history['loss'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa184c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retraining\n",
    "Retraining = True\n",
    "# Retraining = False\n",
    "if Retraining: \n",
    "    def plot_learning_history(history):\n",
    "        fig = plt.figure(1, figsize=(14, 5))\n",
    "        ax = fig.add_subplot(1, 1, 1)\n",
    "        episodes = np.arange(len(history))+1\n",
    "        plt.plot(episodes, history, lw=4,\n",
    "        marker='o', markersize=10)\n",
    "        ax.tick_params(axis='both', which='major', labelsize=15)\n",
    "        plt.xlabel('Episodes', size=20)\n",
    "        plt.ylabel('# Total Rewards', size=20)\n",
    "        plt.show()\n",
    "\n",
    "    ## General settings\n",
    "    EPISODES = 50\n",
    "    batch_size = 32\n",
    "    init_replay_memory_size = 500\n",
    "\n",
    "\n",
    "    env = gym.make('CartPole-v1')\n",
    "    agent = DQNAgent(env)\n",
    "    state, info = env.reset()\n",
    "    state = np.reshape(state, [1, agent.state_size])\n",
    "    ## Filling up the replay-memory\n",
    "    for i in range(init_replay_memory_size):\n",
    "        action = agent.choose_action(state)\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        next_state = np.reshape(next_state, [1, agent.state_size])\n",
    "        \n",
    "        \n",
    "        done = terminated or truncated\n",
    "        \n",
    "        agent.remember(Transition(state, action, reward,\n",
    "        next_state, done))\n",
    "        if terminated or truncated:\n",
    "            state, info = env.reset()\n",
    "            state = np.reshape(state, [1, agent.state_size])\n",
    "        else:\n",
    "            state = next_state\n",
    "\n",
    "\n",
    "    total_rewards, losses = [], []\n",
    "    for e in range(EPISODES):\n",
    "        print('Episode: ', e)\n",
    "        state, info = env.reset()\n",
    "\n",
    "        state = np.reshape(state, [1, agent.state_size])\n",
    "        for i in range(50):\n",
    "            print('Episode_i: ', i)\n",
    "            \n",
    "            \n",
    "            action = agent.choose_action(state)\n",
    "            \n",
    "            \n",
    "            \n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            \n",
    "            done = terminated or truncated\n",
    "            next_state = np.reshape(next_state,\n",
    "                            [1, agent.state_size])\n",
    "            agent.remember(Transition(state, action, reward,\n",
    "                            next_state, done))\n",
    "            \n",
    "            \n",
    "            \n",
    "            state = next_state\n",
    "\n",
    "            if terminated or truncated:\n",
    "                total_rewards.append(i)\n",
    "                print('Episode: %d/%d, Total reward: %d'\n",
    "                % (e, EPISODES, i))\n",
    "                break\n",
    "            loss = agent.replay(batch_size)\n",
    "            \n",
    "            \n",
    "            \n",
    "            losses.append(loss)\n",
    "            \n",
    "    plot_learning_history(total_rewards)\n",
    "\n",
    "    agent.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70cfeafa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-16 08:20:53.169189: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import gym\n",
    "\n",
    "env = gym.make('CartPole-v1', render_mode='human')\n",
    "agent = DQNAgent(env)\n",
    "agent.epsilon = 0\n",
    "state, info = env.reset()\n",
    "state = np.reshape(state, [1, agent.state_size])\n",
    "\n",
    "agent.load_model(39)\n",
    "\n",
    "state, info = env.reset()\n",
    "\n",
    "\n",
    "\n",
    "env.render()\n",
    "state = np.reshape(state, [1, agent.state_size])\n",
    "for i in range(500):\n",
    "    print(i)\n",
    "    action = agent.choose_action(state)\n",
    "    next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "    next_state = np.reshape(next_state,\n",
    "                    [1, agent.state_size])\n",
    "    state = next_state\n",
    "\n",
    "    env.render()\n",
    "#     time.sleep(0.5)\n",
    "    if terminated or truncated:\n",
    "        time.sleep(2)\n",
    "        observation, info = env.reset()\n",
    "#         \n",
    "        break\n",
    "    \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c165bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
