{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91a6d3d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial Q-values ... \n",
      "\n",
      "{(0, 0): {'up': 0, 'down': 0, 'left': 0, 'right': 0}, (0, 1): {'up': 0, 'down': 0, 'left': 0, 'right': 0}, (0, 2): {'up': 0, 'down': 0, 'left': 0, 'right': 0}, (0, 3): {'up': 0, 'down': 0, 'left': 0, 'right': 0}, (1, 0): {'up': 0, 'down': 0, 'left': 0, 'right': 0}, (1, 1): {'up': 0, 'down': 0, 'left': 0, 'right': 0}, (1, 2): {'up': 0, 'down': 0, 'left': 0, 'right': 0}, (1, 3): {'up': 0, 'down': 0, 'left': 0, 'right': 0}, (2, 0): {'up': 0, 'down': 0, 'left': 0, 'right': 0}, (2, 1): {'up': 0, 'down': 0, 'left': 0, 'right': 0}, (2, 2): {'up': 0, 'down': 0, 'left': 0, 'right': 0}, (2, 3): {'up': 0, 'down': 0, 'left': 0, 'right': 0}}\n",
      "reward!\n",
      "penalty!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "penalty!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "penalty!\n",
      "penalty!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "penalty!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "penalty!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "reward!\n",
      "latest Q-values ... \n",
      "\n",
      "{(0, 0): {'up': 0.5598045008412217, 'down': 0.42001675949883666, 'left': 0.5172946962269198, 'right': 0.6813474500809859}, (0, 1): {'up': 0.7366682458183286, 'down': 0.736403346033732, 'left': 0.5883944557425732, 'right': 0.8566564036607588}, (0, 2): {'up': 0.8889017313458658, 'down': 0.740096669996558, 'left': 0.7489352608059027, 'right': 0.9591040799470485}, (0, 3): {'up': 0, 'down': 0, 'left': 0, 'right': 0}, (1, 0): {'up': 0.5138335202348193, 'down': 0.26645508249768723, 'left': 0.3745692012359283, 'right': 0.368898971951269}, (1, 1): {'up': 0, 'down': 0, 'left': 0, 'right': 0}, (1, 2): {'up': 0.8425221240071676, 'down': 0.4228094619538835, 'left': 0.6492144627981186, 'right': 0.7738813377710736}, (1, 3): {'up': 0.9657800932290885, 'down': -250.0, 'left': -333.13513272921335, 'right': -76.54725274725276}, (2, 0): {'up': 0.3516431568329451, 'down': 0.23425083446393588, 'left': 0.24727115998449092, 'right': 0.25778931177428394}, (2, 1): {'up': 0.25274564343327144, 'down': 0.2537890960884334, 'left': 0.2414003674503645, 'right': 0.27799527653408246}, (2, 2): {'up': -29.882976903288572, 'down': -199.87576118883916, 'left': 0.30522749747756656, 'right': -500.0}, (2, 3): {'up': 0, 'down': 0, 'left': 0, 'right': 0}}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set_theme()\n",
    "\n",
    "BOARD_ROWS = 3\n",
    "BOARD_COLS = 4\n",
    "WIN_STATE = (0, 3)\n",
    "LOSE_STATE = (2, 3)\n",
    "\n",
    "REWARD = 1\n",
    "PENALTY = -1000\n",
    "\n",
    "\n",
    "class State:\n",
    "    def __init__(self, state=None):\n",
    "        self.board = np.zeros([BOARD_ROWS, BOARD_COLS])\n",
    "        self.board[1, 1] = -1\n",
    "        if not state:\n",
    "            \n",
    "            self.state = self.random_START()\n",
    "        else:\n",
    "            self.state = state\n",
    "        self.isEnd = False\n",
    "        \n",
    "\n",
    "    def random_START(self):\n",
    "        while True:\n",
    "            i = random.randrange(BOARD_ROWS)\n",
    "            j = random.randrange(BOARD_COLS)\n",
    "            \n",
    "            if (i,j) != (1,1) and (i,j) != WIN_STATE and (i,j) != LOSE_STATE:\n",
    "                return (i,j)\n",
    "\n",
    "    def giveReward(self):\n",
    "        if self.state == WIN_STATE:\n",
    "            print(\"reward!\")\n",
    "            return REWARD\n",
    "        if self.state == LOSE_STATE:\n",
    "            print(\"penalty!\")\n",
    "            return PENALTY\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def isEndFunc(self):\n",
    "        if (self.state == WIN_STATE) or (self.state == LOSE_STATE):\n",
    "            self.isEnd = True\n",
    "\n",
    "    def _chooseActionProb(self, action):\n",
    "        \n",
    "        p0 = [0.8, 0.1, 0.1]\n",
    "        if action == \"up\":\n",
    "            return np.random.choice([\"up\", \"left\", \"right\"], p=p0)\n",
    "        if action == \"down\":\n",
    "            return np.random.choice([\"down\", \"left\", \"right\"], p=p0)\n",
    "        if action == \"left\":\n",
    "            return np.random.choice([\"left\", \"up\", \"down\"], p=p0)\n",
    "        if action == \"right\":\n",
    "            return np.random.choice([\"right\", \"up\", \"down\"], p=p0)\n",
    "\n",
    "    def nxtPosition(self, action):\n",
    "\n",
    "\n",
    "        # non-deterministic\n",
    "        action = self._chooseActionProb(action)\n",
    "\n",
    "        if action == \"up\":\n",
    "            nxtState = (self.state[0] - 1, self.state[1])\n",
    "        elif action == \"down\":\n",
    "            nxtState = (self.state[0] + 1, self.state[1])\n",
    "        elif action == \"left\":\n",
    "            nxtState = (self.state[0], self.state[1] - 1)\n",
    "        else:\n",
    "            nxtState = (self.state[0], self.state[1] + 1)\n",
    "\n",
    "\n",
    "        # if next state is legal\n",
    "        if (nxtState[0] >= 0) and (nxtState[0] <= 2):\n",
    "            if (nxtState[1] >= 0) and (nxtState[1] <= 3):\n",
    "                if nxtState != (1, 1):\n",
    "                    return nxtState\n",
    "        return self.state\n",
    "\n",
    "\n",
    "\n",
    "class Agent:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.states = []  # record position and action taken at the position\n",
    "        self.actions = [\"up\", \"down\", \"left\", \"right\"]\n",
    "        self.State = State()\n",
    "        self.isEnd = self.State.isEnd\n",
    "        self.lr = 0\n",
    "\n",
    "        self.gamma = 0.95\n",
    "        self.k = 1.5\n",
    "        \n",
    "        # initial Q values\n",
    "        self.Q_values = {}\n",
    "        self.Q_values_count = {}\n",
    "        for i in range(BOARD_ROWS):\n",
    "            for j in range(BOARD_COLS):\n",
    "                self.Q_values[(i, j)] = {}\n",
    "                self.Q_values_count[(i, j)] = {}\n",
    "                for a in self.actions:\n",
    "                    self.Q_values[(i, j)][a] = 0  # Q value is a dict of dict\n",
    "                    self.Q_values_count[(i, j)][a] = 0\n",
    "                    \n",
    "                    \n",
    "    def bestAction(self, state):\n",
    "        mx_nxt_reward = 0\n",
    "        for a in self.actions:\n",
    "            \n",
    "\n",
    "            nxt_reward = self.Q_values[state][a]\n",
    "            if nxt_reward >= mx_nxt_reward:\n",
    "                action = a\n",
    "                mx_nxt_reward = nxt_reward\n",
    "        return action, mx_nxt_reward\n",
    "    \n",
    "    def chooseAction(self):\n",
    "\n",
    "        mx_nxt_reward = 0\n",
    "        action = \"\"\n",
    "\n",
    "\n",
    "        Q_a_list = []\n",
    "        current_position = self.State.state\n",
    "        \n",
    "        for a in self.actions:\n",
    "            \n",
    "            \n",
    "            Q_a_list.append(self.Q_values[current_position][a])\n",
    "        \n",
    "        Q_a_array = self.k**np.array(Q_a_list)\n",
    "        Prob_a_givenState = Q_a_array/np.sum(Q_a_array)\n",
    "            \n",
    "        action = random.choices(population=self.actions, weights=Prob_a_givenState,k=1)[0]\n",
    "        \n",
    "       \n",
    "        return action\n",
    "\n",
    "\n",
    "\n",
    "    def takeAction(self, action):\n",
    "        position = self.State.nxtPosition(action)\n",
    "        # update State\n",
    "        return State(state=position)\n",
    "\n",
    "    def reset(self):\n",
    "#         self.states = []\n",
    "        self.State = State()\n",
    "        self.isEnd = self.State.isEnd\n",
    "        \n",
    "    def showValues(self):\n",
    "        \n",
    "        State_matrix = np.zeros((BOARD_ROWS,BOARD_COLS))\n",
    "        for i in range(0, BOARD_ROWS):\n",
    "            for j in range(0, BOARD_COLS):             \n",
    "                _, V = self.bestAction((i,j))\n",
    "                State_matrix[i,j] = V\n",
    "\n",
    "        State_matrix[WIN_STATE[0],WIN_STATE[1]] = REWARD\n",
    "        State_matrix[LOSE_STATE[0],LOSE_STATE[1]] = PENALTY\n",
    "        ax = sns.heatmap(State_matrix, annot=True)\n",
    "\n",
    "    def play(self, rounds=300):\n",
    "        i = 0\n",
    "        for i in range(rounds):\n",
    "\n",
    "            current_state = self.State.state\n",
    "            action = self.chooseAction()\n",
    "            self.State = self.takeAction(action)\n",
    "            \n",
    "            self.Q_values_count[current_state][action] += 1\n",
    "            \n",
    "            \n",
    "            reward = self.State.giveReward()\n",
    "            next_state = self.State.state\n",
    "            \n",
    "            self.State.isEndFunc()\n",
    "            self.isEnd = self.State.isEnd\n",
    "            \n",
    "            _, V = self.bestAction(next_state)\n",
    "            \n",
    "\n",
    "            \n",
    "            if self.State.isEnd:\n",
    "                self.Q_values[current_state][action] = (1-self.lr)* self.Q_values[current_state][action]+ self.lr*(reward)\n",
    "                self.reset()\n",
    "            else:\n",
    "                self.lr = 1/(1+self.Q_values_count[current_state][action])\n",
    "                self.Q_values[current_state][action] = (1-self.lr)* self.Q_values[current_state][action]+ self.lr*(reward + self.gamma*V)\n",
    "            \n",
    "            \n",
    "            \n",
    "\n",
    "            \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ag = Agent()\n",
    "    print(\"initial Q-values ... \\n\")\n",
    "    print(ag.Q_values)\n",
    "\n",
    "    ag.play(10000)\n",
    "    print(\"latest Q-values ... \\n\")\n",
    "    print(ag.Q_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36810bb4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
